[databricks-certified-data-engineer-associate-exam-guide.pdf](file:///C:/Users/rturek2/Downloads/databricks-certified-data-engineer-associate-exam-guide.pdf)

# Section 1: Databricks Lakehouse Platform 
● Describe the relationship between the data lakehouse and the data warehouse. 
● Identify the improvement in data quality in the data lakehouse over the data lake. 
● Compare and contrast silver and gold tables, which workloads will use a bronze table as a source, which workloads will use a gold table as a source. 
● Identify elements of the Databricks Platform Architecture, such as what is located in the data plane versus the control plane and what resides in the customer’s cloud account 
● Differentiate between all-purpose clusters and jobs clusters. 
● Identify how cluster software is versioned using the Databricks Runtime. 
● Identify how clusters can be filtered to view those that are accessible by the user. 
● Describe how clusters are terminated and the impact of terminating a cluster. 
● Identify a scenario in which restarting the cluster will be useful. 
● Describe how to use multiple languages within the same notebook. 
● Identify how to run one notebook from within another notebook. 
● Identify how notebooks can be shared with others. 
● Describe how Databricks Repos enables CI/CD workflows in Databricks. 
● Identify Git operations available via Databricks Repos. 
● Identify limitations in Databricks Notebooks version control functionality relative to Repos. 

✅

| ✅   | Question                                               | Answer                                                                                                                                                                                                            |
| --- | ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ✅   | Relationship between data lakehouse and data warehouse | Combines the flexibility of a data lake (schema-on-read) with the data governance and structure of a data warehouse (schema-on-write).                                                                            |
| ✅   | Improvement in data quality                            | Data lakehouse enforces data quality checks and transformation steps during ingestion, leading to cleaner data than a raw data lake.                                                                              |
| ✅   | Silver vs. Gold tables                                 | Silver tables: Near real-time, may contain some inconsistencies. Gold tables: Fully refined and validated data, suitable for analytics.                                                                           |
| ✅   | Bronze table source workloads                          | Data ingestion pipelines, initial data exploration.                                                                                                                                                               |
| ✅   | Gold table source workloads                            | Reporting, machine learning, business intelligence.                                                                                                                                                               |
| ✅   | Databricks Platform Architecture                       | - **Data Plane:** Handles data processing (e.g., Spark clusters). Resides in customer's cloud account. - **Control Plane:** Manages clusters, users, notebooks (Databricks Service). Resides in Databricks cloud. |
| ✅   | All-purpose clusters vs. Job clusters                  | All-purpose: General-purpose workloads (e.g., exploration, development). Job clusters: Specific jobs requiring high performance or cost optimization.                                                             |
| ✅   | Databricks Runtime versions clusters                   | Provides access to Apache Spark versions, libraries, and configurations.                                                                                                                                          |
| ✅   | Filtering clusters                                     | Use the clusters UI filters based on properties like creator, status, or cluster type.                                                                                                                            |
| ✅   | Cluster termination and impact                         | Terminates running jobs and releases resources. Costs stop accruing.                                                                                                                                              |
| ✅   | Scenario for cluster restart                           | When a job fails due to software glitches or temporary issues, restarting the cluster can resolve them.                                                                                                           |
| ✅   | Using multiple languages in notebooks                  | Magic commands (%lang) specify the language for specific code cells within a notebook.                                                                                                                            |
| ✅   | Running one notebook from another                      | Use the `dbutils.notebook.run` function to call another notebook from within the current one.                                                                                                                     |
| ✅   | Sharing notebooks                                      | Share notebooks with other users or groups by setting permissions in the notebooks UI.                                                                                                                            |
| ✅   | Databricks Repos and CI/CD                             | Enables version control, code reviews, and automated testing/deployment for notebooks and data pipelines.                                                                                                         |
| ✅   | Git operations in Databricks Repos                     | Clone, commit, push, pull, branch, merge (similar to standard Git).                                                                                                                                               |
| ✅   | Limitations of notebook version control                | Notebooks lack advanced branching, merging capabilities compared to dedicated Git repositories.                                                                                                                   |

# Section 2: ELT with Apache Spark 
● Extract data from a single file and from a directory of files 
● Identify the prefix included after the FROM keyword as the data type. 
● Create a view, a temporary view, and a CTE as a reference to a file 
● Identify that tables from external sources are not Delta Lake tables. 
● Create a table from a JDBC connection and from an external CSV file 
● Identify how the count_if function and the count where x is null can be used 
● Identify how the count(row) skips NULL values. 
● Deduplicate rows from an existing Delta Lake table. 
● Create a new table from an existing table while removing duplicate rows. 
● Deduplicate a row based on specific columns. 
● Validate that the primary key is unique across all rows. 
● Validate that a field is associated with just one unique value in another field. ● Validate that a value is not present in a specific field. ● Cast a column to a timestamp. 
● Extract calendar data from a timestamp. 
● Extract a specific pattern from an existing string column. ● Utilize the dot syntax to extract nested data fields. 
● Identify the benefits of using array functions. 
● Parse JSON strings into structs. 
● Identify which result will be returned based on a join query. 
● Identify a scenario to use the explode function versus the flatten function ● Identify the PIVOT clause as a way to convert data from wide format to a long format. 
● Define a SQL UDF. 
● Identify the location of a function. 
● Describe the security model for sharing SQL UDFs. 
● Use CASE/WHEN in SQL code. 
● Leverage CASE/WHEN for custom control flow. 

| ✅   | Question                                  | Answer                                                                                                                                                             |
| --- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ✅   | Extract data from a single file/directory | Use `spark.read.format("option").load("path")`. "option" depends on file format (e.g., csv, parquet).                                                              |
| ✅   | Data type prefix after FROM               | Yes, prefix like "csv", "parquet" indicates the file format being read.                                                                                            |
| ✅   | View vs. Temporary View vs. CTE           | - View: Persistent schema, referenced by name in queries. - Temporary View: Exists only for the Spark session. - CTE: Named result set used within a single query. |
|     | Tables from external sources              | Not Delta Lake tables by default. Use Delta Lake APIs for Delta tables.                                                                                            |
| ✅   | Create table from JDBC/CSV                | Use `spark.read.jdbc(...)` for JDBC and `spark.read.format("csv").load(...)` for CSV.                                                                              |
| ✅   | count_if vs. count where null             | - `count_if(condition)`: Counts rows where the condition is true. - `count(where x is null)`: Counts rows where x is null.                                         |
| ✅   | count(row) and null values                | `count(row)` counts all rows, including rows with null values.                                                                                                     |
| ✅   | Deduplicate Delta Lake table              | Use `table.dropDuplicates()` or SQL syntax `SELECT DISTINCT ...`.                                                                                                  |
| ✅   | New table without duplicates              | Use `spark.table("source").select(...).distinct().createOrReplaceTempView("new_table")`.                                                                           |
| ✅   | Deduplicate based on specific columns     | Use `SELECT DISTINCT col1, col2, ... FROM table`.                                                                                                                  |
| ✅   | Validate unique primary key               | Use `SELECT COUNT(*) FROM table GROUP BY primary_key HAVING COUNT(*) > 1`.                                                                                         |
| ✅   | Validate one-to-one relationship          | Use `SELECT COUNT(*) FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id GROUP BY t2.other_field HAVING COUNT(*) > 1`.                                                  |
| ✅   | Validate value absence                    | Use `SELECT * FROM table WHERE field NOT IN ('value1', 'value2')`.                                                                                                 |
| ✅   | Cast column to timestamp                  | Use `col("column").cast("timestamp")`.                                                                                                                             |
| ✅   | Extract calendar data                     | Use functions like `year`, `month`, `dayofmonth` from the timestamp column.                                                                                        |
| ✅   | Extract pattern from string               | Use regular expressions with `regexp_extract` function.                                                                                                            |
| ✅   | Nested data with dot syntax               | Access nested fields using dot notation (e.g., `data.nested.field`).                                                                                               |
| ✅   | Benefits of array functions               | Simplify working with arrays, filtering, transforming elements.                                                                                                    |
| ✅   | Parse JSON strings                        | Use `spark.read.json(...)` or `to_json` and `from_json` functions.                                                                                                 |
| ✅   | Join query results                        | Depends on join type (inner, left, right, full). Inner join returns rows where both tables have matching values.                                                   |
| ✅   | Explode vs. Flatten                       | - `explode`: Creates a new row for each element in an array. - `flatten`: Collapses nested structures into a single level.                                         |
| ✅   | PIVOT clause                              | Transforms data from wide format (many columns) to long format (fewer columns with aggregated values).                                                             |
| ✅   | SQL UDF definition                        | Use `CREATE FUNCTION` statement with function name, parameters, return type, and function body.                                                                    |
| ✅   | Function location                         | Functions can be built-in (Spark SQL), user-defined (UDF), or registered from external libraries.                                                                  |
| ✅   | UDF sharing security                      | UDFs are visible only to the session that created them unless explicitly registered with `spark.udf.register(...)`.                                                |
| ✅   | CASE/WHEN in SQL code                     | Conditional logic, assigning values based on conditions.                                                                                                           |
| ✅   | CASE/WHEN for custom control flow         | Enables complex decision-making within SQL queries.                                                                                                                |

# Section 3: Incremental Data Processing 
● Identify where Delta Lake provides ACID transactions 
● Identify the benefits of ACID transactions. 
● Identify whether a transaction is ACID-compliant. 
● Compare and contrast data and metadata. 
● Compare and contrast managed and external tables. 
● Identify a scenario to use an external table. 
● Create a managed table. 
● Identify the location of a table. 
● Inspect the directory structure of Delta Lake files.
● Identify who has written previous versions of a table. 
● Review a history of table transactions. ● Roll back a table to a previous version. 
● Identify that a table can be rolled back to a previous version. 
● Query a specific version of a table. 
● Identify why Zordering is beneficial to Delta Lake tables. 
● Identify how vacuum commits deletes. 
● Identify the kind of files Optimize compacts. 
● Identify CTAS as a solution. 
● Create a generated column. 
● Add a table comment. 
● Use CREATE OR REPLACE TABLE and INSERT OVERWRITE 
● Compare and contrast CREATE OR REPLACE TABLE and INSERT OVERWRITE 
● Identify a scenario in which MERGE should be used. ● Identify MERGE as a command to deduplicate data upon writing. 
● Describe the benefits of the MERGE command. 
● Identify why a COPY INTO statement is not duplicating data in the target table. 
● Identify a scenario in which COPY INTO should be used. 
● Use COPY INTO to insert data. 
● Identify the components necessary to create a new DLT pipeline. 
● Identify the purpose of the target and of the notebook libraries in creating a pipeline. 
● Compare and contrast triggered and continuous pipelines in terms of cost and latency 
● Identify which source location is utilizing Auto Loader. 
● Identify a scenario in which Auto Loader is beneficial. 
● Identify why Auto Loader has inferred all data to be STRING from a JSON source 
● Identify the default behavior of a constraint violation 
● Identify the impact of ON VIOLATION DROP ROW and ON VIOLATION FAIL UPDATEfor a constraint violation ● Explain change data capture and the behavior of APPLY CHANGES INTO 
● Query the events log to get metrics, perform audit loggin, examine lineage. 
● Troubleshoot DLT syntax: Identify which notebook in a DLT pipeline produced an error, identify the need for LIVE in create statement, identify the need for STREAM in from clause. 

| ✅   | Question                                         | Answer                                                                                                                                                                                                                       |
| --- | ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ✅   | Delta Lake ACID transactions                     | Provides Atomicity, Consistency, Isolation, Durability guarantees for data updates.                                                                                                                                          |
| ✅   | Benefits of ACID transactions                    | Ensures data integrity, prevents partial updates, and allows rollbacks.                                                                                                                                                      |
| ✅   | Identifying ACID-compliant transactions          | Check for all ACID properties (Atomicity, Consistency, Isolation, Durability) being supported.                                                                                                                               |
| ✅   | Data vs. Metadata                                | - Data: The actual content of the table (records, columns). - Metadata: Information about the data (schema, ownership, versions).                                                                                            |
| ✅   | Managed vs. External tables                      | - Managed: Delta Lake owns and manages the data files. - External: Delta Lake references data files stored elsewhere.                                                                                                        |
| ✅   | Scenario for external table                      | Use external tables when data resides in a separate system and Delta Lake needs to reference it without management.                                                                                                          |
| ✅   | Create a managed table                           | Use `CREATE TABLE ... USING DELTA LOCATION ...`.                                                                                                                                                                             |
| ✅   | Table location                                   | Use `SHOW LOCATION TABLE ...`.                                                                                                                                                                                               |
| ✅   | Inspect Delta Lake directory structure           | Use tools like `dbfs ls` to view nested folders containing data parquet files and metadata checkpoints.                                                                                                                      |
| ✅   | Identify who wrote previous versions             | Use `DESCRIBE HISTORY TABLE ...`.                                                                                                                                                                                            |
| ✅   | Review table transaction history                 | Use `DESCRIBE HISTORY TABLE ...`.                                                                                                                                                                                            |
| ✅   | Rollback table to a previous version             | Use `ALTER TABLE ... SET VERSION AS ...`.                                                                                                                                                                                    |
| ✅   | Query a specific table version                   | Use `READ VERSION AS ...` clause in your query.                                                                                                                                                                              |
| ✅   | Zordering benefit                                | Improves query performance by organizing data files based on frequently used columns.                                                                                                                                        |
| ✅   | Vacuum commits and deletes                       | Vacuum commits reclaim unused space from deleted data versions.                                                                                                                                                              |
| ✅   | Optimize compacts                                | Optimize compacts rewrite data files for better performance, potentially creating fewer, larger files.                                                                                                                       |
| ✅   | CTAS (CREATE TABLE AS SELECT)                    | Creates a new table by running a SELECT query on existing data.                                                                                                                                                              |
| ✅   | Generate a column                                | Use `ALTER TABLE ... ADD COLUMNS ...` with an expression to define the generated column.                                                                                                                                     |
| ✅   | Add a table comment                              | Use `ALTER TABLE ... SET COMMENT ...`.                                                                                                                                                                                       |
| ✅   | CREATE OR REPLACE TABLE vs. INSERT OVERWRITE     | - CREATE OR REPLACE TABLE: Drops and recreates the table entirely. - INSERT OVERWRITE: Overwrites existing data based on the insert logic.                                                                                   |
| ✅   | Scenario for MERGE                               | Use MERGE for complex data updates involving inserts, updates, and deletes based on conditions.                                                                                                                              |
| ✅   | MERGE for deduplication                          | MERGE can include a deduplication step by specifying a unique key and using appropriate actions (e.g., INSERT, IGNORE) for duplicate rows.                                                                                   |
| ✅   | Benefits of MERGE                                | Simplifies complex data updates, reduces code duplication, and potentially improves performance.                                                                                                                             |
| ✅   | COPY INTO not duplicating data                   | Check for existing data with matching keys causing updates instead of inserts (use INSERT instead of COPY INTO for full inserts).                                                                                            |
| ✅   | Scenario for COPY INTO                           | Use COPY INTO for efficient data loading from external sources when duplicates are not a concern.                                                                                                                            |
| ✅   | Insert data with COPY INTO                       | Use `COPY INTO ... FROM ...`.                                                                                                                                                                                                |
| ✅   | DLT pipeline components                          | - Target: Delta table to store the processed data. - Notebook libraries: Code for data transformation and loading.                                                                                                           |
| ✅   | Target and notebook library purpose              | - Target: Defines the destination for processed data. - Notebook libraries: Contain the logic for data processing.                                                                                                           |
| ✅   | Triggered vs. continuous pipelines               | - Triggered: Runs based on a schedule or event. Lower cost but higher latency. - Continuous: Runs constantly, processing new data as it arrives. Higher cost but lower latency.                                              |
| ✅   | Auto Loader source location                      | Auto Loader monitors a specific directory for new data files.                                                                                                                                                                |
| ✅   | Benefit of Auto Loader                           | Auto Loader automatically ingests new data files into a Delta table as they arrive.                                                                                                                                          |
| ✅   | Auto Loader inferring all data as STRING         | Configure the schema or use data type inference options within Auto Loader to handle non-string data types.                                                                                                                  |
| ✅   | Default constraint violation behavior            | By default, constraint violations raise an exception.                                                                                                                                                                        |
|     | ON VIOLATION DROP ROW vs. FAIL UPDATE            | - ON VIOLATION DROP ROW: Ignores and drops rows violating the constraint. - ON VIOLATION FAIL UPDATE: Raises an exception for constraint violations during updates.                                                          |
| ✅   | Change data capture (CDC) and APPLY CHANGES INTO | CDC tracks data modifications (inserts, updates, deletes). APPLY CHANGES INTO replays these changes on another table.                                                                                                        |
|     | Events log for metrics, auditing, lineage        | The Delta Lake events log stores information about table operations, enabling metrics tracking, audit logging, and data lineage analysis.                                                                                    |
|     | DLT syntax troubleshooting                       | - Identify error notebook: Look for errors in the notebook execution history. - Identify LIVE in CREATE: The LIVE option might be required for streaming data sources. - Identify STREAM in FROM: The STREAM option might be |


# Section 4: Production Pipelines 
● Identify the benefits of using multiple tasks in Jobs. 
● Set up a predecessor task in Jobs. 
● Identify a scenario in which a predecessor task should be set up. 
● Review a task's execution history. 
● Identify CRON as a scheduling opportunity. 
● Debug a failed task. 
● Set up a retry policy in case of failure. 
● Create an alert in the case of a failed task. 
● Identify that an alert can be sent via email. 

| ✅   | Question                           | Answer                                                                                                                                                              |
| --- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ✅   | Benefits of multiple tasks in Jobs | - Modularize complex workflows into smaller, reusable steps. - Improve maintainability and code organization. - Allow for parallel execution of independent tasks.  |
| ✅   | Setting up a predecessor task      | Use the "Depends On" option in the task configuration to specify a completed task as a prerequisite.                                                                |
| ✅   | Scenario for predecessor task      | Use predecessor tasks when one task relies on the output or successful completion of another task before it can start. (e.g., data validation before data loading). |
| ✅   | Reviewing task execution history   | Access the Jobs UI and view the individual task history for details on success, failure, and logs.                                                                  |
| ✅   | CRON scheduling                    | Enables scheduling tasks to run periodically based on CRON expressions (e.g., hourly, daily at specific times).                                                     |
| ✅   | Debugging a failed task            | Review the task's logs for error messages and identify the root cause of the failure.                                                                               |
| ✅   | Setting up a retry policy          | Configure retries for failed tasks with a specified number of attempts and delay between retries.                                                                   |
| ✅   | Creating an alert for failed tasks | Set up job notifications to receive alerts (e.g., email) when tasks fail.                                                                                           |
| ✅   | Email alerts for failures          | Job notifications can be configured to send email alerts upon task failures.                                                                                        |


# Section 5: Data Governance 
● Identify one of the four areas of data governance. 
● Compare and contrast metastores and catalogs. 
● Identify Unity Catalog securables. 
● Define a service principal. 
● Identify the cluster security modes compatible with Unity Catalog. 
● Create a UC-enabled all-purpose cluster. 
● Create a DBSQL warehouse. 
● Identify how to query a three-layer namespace. 
● Implement data object access control 
● Identify colocating metastores with a workspace as best practice. 
● Identify using service principals for connections as best practice. 
● Identify the segregation of business units across catalog as best practice.

| ✅  | Question                                  | Answer                                                                                                                                                                                            |
| --- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|     | Area of data governance                   | - Data quality: Ensuring data accuracy, completeness, and consistency.                                                                                                                            |
|     | Metastores vs. Catalogs                   | - **Metastores:** Store metadata about data location, schema, and lineage. (e.g., Hive Metastore) - **Catalogs:** Provide a unified view of data across various metastores and storage locations. |
|     | Unity Catalog securables                  | Data objects (tables, views), databases, locations.                                                                                                                                               |
|     | Service principal                         | An identity used by applications (like Databricks) to access Azure resources (like Azure Data Lake Storage) securely.                                                                             |
|     | Cluster security modes with Unity Catalog | - ACL (Access Control Lists) - RBAC (Role-Based Access Control) with Azure Active Directory (AAD).                                                                                                |
|     | Create UC-enabled all-purpose cluster     | During cluster creation, enable "Use Unity Catalog" and select appropriate security options.                                                                                                      |
|     | Create a DBSQL warehouse                  | Use the DBSQL UI to configure and create a warehouse for interactive SQL workloads.                                                                                                               |
|     | Querying a three-layer namespace          | Use the fully qualified name with database, schema (if applicable), and object name (e.g., `dbName.schemaName.tableName`).                                                                        |
|     | Data object access control                | Use Unity Catalog permissions to grant/revoke access to data objects for users and groups.                                                                                                        |
|     | Co-locating metastores with workspace     | Not necessarily the best practice anymore. Unity Catalog manages metadata centrally.                                                                                                              |
|     | Service principals for connections        | Best practice for secure connections to external data sources (e.g., Azure Data Lake Storage) from Databricks.                                                                                    |
|     | Segregation of business units             | Best practice to logically separate data access and ownership for different business units within the same Unity Catalog.<br><br>pen_spark                                                        |



Sample Questions These questions are retired from a previous version of the exam. The purpose is to show you objectives as they are stated on the exam guide, and give you a sample question that aligns to the objective. The exam guide lists the objectives that could be covered on an exam. The best way to prepare for a certification exam is to review the exam outline in the exam guide. Question 1 Objective: Describe the benefits of a data lakehouse over a traditional data warehouse. What is a benefit of a data lakehouse that is unavailable in a traditional data warehouse? A. A data lakehouse provides a relational system of data management. B. A data lakehouse captures snapshots of data for version control purposes. C. A data lakehouse couples storage and compute for complete control. D. A data lakehouse utilizes proprietary storage formats for data. E. A data lakehouse enables both batch and streaming analytics. Question 2 Objective: Identify query optimization techniques A data engineering team needs to query a Delta table to extract rows that all meet the same condition. However, the team has noticed that the query is running slowly. The team has already tuned the size of the data files. Upon investigating, the team has concluded that the rows meeting the condition are sparsely located throughout each of the data files. Which optimization techniques could speed up the query? A. Data skipping B. Z-Ordering C. Bin-packing D. Write as a Parquet file E. Tuning the file size Question 3 Objective: Identify data workloads that utilize a Silver table as its source. Which data workload will utilize a Silver table as its source? A. A job that enriches data by parsing its timestamps into a human-readable format B. A job that queries aggregated data that already feeds into a dashboard C. A job that ingests raw data from a streaming source into the Lakehouse D. A job that aggregates cleaned data to create standard summary statistics E. A job that cleans data by removing malformatted records Question 4 Objective: Describe how to configure a refresh schedule An engineering manager uses a Databricks SQL query to monitor their team’s progress on fixes related to customer-reported bugs. The manager checks the results of the query every day, but they are manually rerunning the query each day and waiting for the results. How should the query be scheduled to ensure the results of the query are updated each day? A. To refresh every 12 hours from the query’s page in Databricks SQL. B. To refresh every 1 day from the query’s page in Databricks SQL. C. To run every 12 hours from the Jobs UI. D. To refresh every 1 day from the SQL warehouse page in Databricks SQL. E. To refresh every 12 hours from the SQL warehouse page in Databricks SQL. Question 5 Objective: Identify commands for granting appropriate permissions A new data engineer has started at a company. The data engineer has recently been added to the company’s Databricks workspace as new.engineer@company.com. The data engineer needs to be able to query the table sales in the database retail. The new data engineer already has been granted USAGE on the database retail. Which command should be used to grant the appropriate permissions to the new data engineer? A. GRANT USAGE ON TABLE sales TO new.engineer@company.com; B. GRANT CREATE ON TABLE sales TO new.engineer@company.com; C. GRANT SELECT ON TABLE sales TO new.engineer@company.com; D. GRANT USAGE ON TABLE new.engineer@company.com TO sales; E. GRANT SELECT ON TABLE new.engineer@company.com TO sales; Answers Question 1: E Question 2: B Question 3: D Question 4: B Question 5: C