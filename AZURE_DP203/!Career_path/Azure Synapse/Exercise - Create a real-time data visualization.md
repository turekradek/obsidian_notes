# Create a realtime report with Azure Stream Analytics and Microsoft Power BI

Data analytics solutions often include a requirement to ingest and processÂ _streams_Â of data. Stream processing differs from batch processing in that streams are generallyÂ _boundless_Â - in other words they are continuous sources of data that must be processed perpetually rather than at fixed intervals.

Azure Stream Analytics provides a cloud service that you can use to define aÂ _query_Â that operates on a stream of data from a streaming source, such as Azure Event Hubs or an Azure IoT Hub. You can use an Azure Stream Analytics query to process a stream of data and send the results directly to Microsoft Power BI for realtime visualization.

In this exercise, youâ€™ll use Azure Stream Analytics to process a stream of sales order data, such as might be generated from an online retail application. The order data will be sent to Azure Event Hubs, from where your Azure Stream Analytics job will read and summarize the data before sending it to Power BI, where you will visualize the data in a report.

This exercise should take approximatelyÂ **45**Â minutes to complete.

## Before you start

Youâ€™ll need anÂ [Azure subscription](https://azure.microsoft.com/free)Â in which you have administrative-level access.

Youâ€™ll also need access to the Microsoft Power BI service. Your school or organization may already provide this, or you canÂ [sign up for the Power BI service as an individual](https://learn.microsoft.com/power-bi/fundamentals/service-self-service-signup-for-power-bi).

## Provision Azure resources

In this exercise, youâ€™ll need an Azure Synapse Analytics workspace with access to data lake storage and a dedicated SQL pool. Youâ€™ll also need an Azure Event Hubs namespace to which the streaming order data can be sent.

Youâ€™ll use a combination of a PowerShell script and an ARM template to provision these resources.

1. Sign into theÂ [Azure portal](https://portal.azure.com/)Â atÂ `https://portal.azure.com`.
2. Use theÂ **[>_]**Â button to the right of the search bar at the top of the page to create a new Cloud Shell in the Azure portal, selecting aÂ **_PowerShell_**Â environment and creating storage if prompted. The cloud shell provides a command line interface in a pane at the bottom of the Azure portal, as shown here:
    
    [![A screenshot of the Azure portal with a cloud shell pane.](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)
    
    > **Note**: If you have previously created a cloud shell that uses aÂ _Bash_Â environment, use the the drop-down menu at the top left of the cloud shell pane to change it toÂ **_PowerShell_**.
    
3. Note that you can resize the cloud shell by dragging the separator bar at the top of the pane, or by using theÂ **â€”**,Â **â—»**, andÂ **X**Â icons at the top right of the pane to minimize, maximize, and close the pane. For more information about using the Azure Cloud Shell, see theÂ [Azure Cloud Shell documentation](https://docs.microsoft.com/azure/cloud-shell/overview).
    
4. In the PowerShell pane, enter the following commands to clone the repo containing this exercise:
    
    CodeCopy
    
    ```
     rm -r dp-203 -f
     git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```
    
5. After the repo has been cloned, enter the following commands to change to the folder for this exercise and run theÂ **setup.ps1**Â script it contains:
    
    CodeCopy
    
    ```
     cd dp-203/Allfiles/labs/19
     ./setup.ps1
    ```
    
6. If prompted, choose which subscription you want to use (this will only happen if you have access to multiple Azure subscriptions).
    
7. While you are waiting for the script to complete, continue with the next task.

## Create a Power BI workspace

In the Power BI service, you organize datasets, reports, and other resources inÂ _workspaces_. Every Power BI user has a default workspace namedÂ **My Workspace**, which you can use in this exercise; but itâ€™s generally good practice to create a workspace for each discrete reporting solution you want to manage.

1. Sign into the Power BI service atÂ [https://app.powerbi.com/](https://app.powerbi.com/)Â using your Power BI service credentials.
2. In the menu bar on the left, selectÂ **Workspaces**Â (the icon looks similar to ğŸ—‡).
3. Create a new workspace with a meaningful name (for example,Â _mslearn-streaming_), selecting theÂ **Pro**Â licensing mode.
    
    > **Note**: If you are using a trial account, you may need to enable additional trial features.
    
4. When viewing your workspace, note its globally unique identifier (GUID) in the page URL (which should be similar toÂ `https://app.powerbi.com/groups/<GUID>/list`). You will need this GUID later.

## Use Azure Stream Analytics to process streaming data

An Azure Stream Analytics job defines a perpetual query that operates on streaming data from one or more inputs and sends the results to one or more outputs.

### Create a Stream Analytics job

1. Switch back to the browser tab containing the Azure portal, and when the script has finished, note the region where yourÂ **dp203-_xxxxxxx_**Â resource group was provisioned.
2. On theÂ **Home**Â page of the Azure portal, selectÂ **+ Create a resource**Â and search forÂ `Stream Analytics job`. Then create aÂ **Stream Analytics job**Â with the following properties:
    - **Subscription**: Your Azure subscription
    - **Resource group**: Select the existingÂ **dp203-_xxxxxxx_**Â resource group.
    - **Name**:Â `stream-orders`
    - **Region**: Select the region where your Synapse Analytics workspace is provisioned.
    - **Hosting environment**: Cloud
    - **Streaming units**: 1
3. Wait for deployment to complete and then go to the deployed Stream Analytics job resource.

### Create an input for the event data stream

1. On theÂ **stream-orders**Â overview page, select theÂ **Inputs**Â page, and use theÂ **Add input**Â menu to add anÂ **Event Hub**Â input with the following properties:
    - **Input alias**:Â `orders`
    - **Select Event Hub from your subscriptions**: Selected
    - **Subscription**: Your Azure subscription
    - **Event Hub namespace**: Select theÂ **events_xxxxxxx_**Â Event Hubs namespace
    - **Event Hub name**: Select the existingÂ **eventhub_xxxxxxx_**Â event hub.
    - **Event Hub consumer group**: Select the existingÂ **$Default**Â consumer group
    - **Authentication mode**: Create system assigned managed identity
    - **Partition key**:Â _Leave blank_
    - **Event serialization format**: JSON
    - **Encoding**: UTF-8
2. Save the input and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create an output for the Power BI workspace

1. View theÂ **Outputs**Â page for theÂ **stream-orders**Â Stream Analytics job. Then use theÂ **Add output**Â menu to add anÂ **Power BI**Â output with the following properties:
    - **Output alias**:Â `powerbi-dataset`
    - **Select Power BI settings manually**: Selected
    - **Group workspace**:Â _The GUID for your workspace_
    - **Authentication mode**:Â _Select_Â **User token**Â _and then use the_Â **Authorize**Â _button at the bottom to sign into your Power BI account_
    - **Dataset name**:Â `realtime-data`
    - **Table name**:Â `orders`
2. Save the output and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create a query to summarize the event stream

1. View theÂ **Query**Â page for theÂ **stream-orders**Â Stream Analytics job.
2. Modify the default query as follows:
    
    CodeCopy
    
    ```
     SELECT
         DateAdd(second,-5,System.TimeStamp) AS StartTime,
         System.TimeStamp AS EndTime,
         ProductID,
         SUM(Quantity) AS Orders
     INTO
         [powerbi-dataset]
     FROM
         [orders] TIMESTAMP BY EventEnqueuedUtcTime
     GROUP BY ProductID, TumblingWindow(second, 5)
     HAVING COUNT(*) > 1
    ```
    
    Observe that this query uses theÂ **System.Timestamp**Â (based on theÂ **EventEnqueuedUtcTime**Â field) to define the start and end of each 5 secondÂ _tumbling_Â (non-overlapping sequential) window in which the total quantity for each product ID is calculated.
    
3. Save the query.

### Run the streaming job to process order data

1. View theÂ **Overview**Â page for theÂ **stream-orders**Â Stream Analytics job, and on theÂ **Properties**Â tab review theÂ **Inputs**,Â **Query**,Â **Outputs**, andÂ **Functions**Â for the job. If the number ofÂ **Inputs**Â andÂ **Outputs**Â is 0, use theÂ **â†» Refresh**Â button on theÂ **Overview**Â page to display theÂ **orders**Â input andÂ **powerbi-dataset**Â output.
2. Select theÂ **â–· Start**Â button, and start the streaming job now. Wait until you are notified that the streaming job started successfully.
3. Re-open the cloud shell pane and run the following command to submit 100 orders.
    
    CodeCopy
    
    ```
     node ~/dp-203/Allfiles/labs/19/orderclient
    ```
    
4. While the order client app is running, switch to the Power BI app browser tab and view your workspace.
5. Refresh the Power BI app page until you see theÂ **realtime-data**Â dataset in your workspace. This dataset is generated by the Azure Stream Analytics job.

## Visualize the streaming data in Power BI

Now that you have a dataset for the streaming order data, you can create a Power BI dashboard that represents it visually.

1. Return to your PowerBI browser tab.
    
2. In theÂ **+ New**Â drop-down menu for your workspace, selectÂ **Dashboard**, and create a new dashboard namedÂ **Order Tracking**.
    
3. On theÂ **Order Tracking**Â dashboard, select theÂ **âœï¸ Edit**Â menu, then selectÂ **+ Add a tile**. Then in theÂ **Add a tile**Â pane, selectÂ **Custom Streaming Data**Â and selectÂ **Next**:
    
4. In theÂ **Add a custom streaming data tile**Â pane, underÂ **Your datasets**, select theÂ **realtime-data**Â dataset, and selectÂ **Next**.
    
5. Change the default visualization type toÂ **Line chart**. Then set the following properties and selectÂ **Next**:
    - **Axis**: EndTime
    - **Value**: Orders
    - **Time window to display**: 1 Minute
6. On theÂ **Tile details**Â pane, set theÂ **Title**Â toÂ **Real-time Order Count**Â and selectÂ **Apply**.
    
7. Switch back to the browser tab containing the Azure portal, and if necessary, re-open the cloud shell pane. Then re-run the following command to submit another 100 orders.
    
    CodeCopy
    
    ```
     node ~/dp-203/Allfiles/labs/19/orderclient
    ```
    
8. While the order submission script is running, switch back to the browser tab containing theÂ **Order Tracking**Â Power BI dashboard and observe that the visualization updates to reflect the new order data as it is processed by the Stream Analytics job (which should still be running).
    
    [![A screenshot of a Power BI report showing a realtime stream of order data.](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/powerbi-line-chart.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/powerbi-line-chart.png)
    
    You can re-run theÂ **orderclient**Â script and observe the data being captured in the real-time dashboard.
    

## Delete resources

If youâ€™ve finished exploring Azure Stream Analytics and Power BI, you should delete the resources youâ€™ve created to avoid unnecessary Azure costs.

1. Close the browser tab containing the Power BI report. Then in theÂ **Workspaces**Â pane, in theÂ **â‹®**Â menu for your workspace selectÂ **Workspace settings**Â and delete the workspace.
2. Return to the browser tab containing the Azure Portal, close the cloud shell pane, and use theÂ **ğŸ—† Stop**Â button to stop the Stream Analytics job. Wait for the notification that the Stream Analytics job has stopped successfully.
3. On the Azure portal, on theÂ **Home**Â page, selectÂ **Resource groups**.
4. Select theÂ **dp203-_xxxxxxx_**Â resource group containing your Azure Event Hub and Stream Analytics resources.
5. At the top of theÂ **Overview**Â page for your resource group, selectÂ **Delete resource group**.
6. Enter theÂ **dp203-_xxxxxxx_**Â resource group name to confirm you want to delete it, and selectÂ **Delete**.
    
    After a few minutes, the resources created in this exercise will be deleted.