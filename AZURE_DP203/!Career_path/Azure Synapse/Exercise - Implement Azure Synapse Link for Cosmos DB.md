# Use Azure Synapse Link for Azure Cosmos DB

Azure Synapse Link for Azure Cosmos DB is a cloud-nativeÂ _hybrid transactional analytical processing_Â (HTAP) technology that enables you to run near-real-time analytics over operational data stored in Azure Cosmos DB from Azure Synapse Analytics.

This exercise should take approximatelyÂ **35**Â minutes to complete.

## Before you start

Youâ€™ll need anÂ [Azure subscription](https://azure.microsoft.com/free)Â in which you have administrative-level access.

## Provision Azure resources

To explore Azure Synapse Link for Azure Cosmos DB, youâ€™ll need an Azure Synapse Analytics workspace and an Azure Cosmos DB account. In this exercise, youâ€™ll use a combination of a PowerShell script and an ARM template to provision these resources in your Azure subscription.

1. Sign into theÂ [Azure portal](https://portal.azure.com/)Â atÂ `https://portal.azure.com`.
2. Use theÂ **[>_]**Â button to the right of the search bar at the top of the page to create a new Cloud Shell in the Azure portal, selecting aÂ **_PowerShell_**Â environment and creating storage if prompted. The cloud shell provides a command line interface in a pane at the bottom of the Azure portal, as shown here:
    
    [![Azure portal with a cloud shell pane](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)
    
    > **Note**: If you have previously created a cloud shell that uses aÂ _Bash_Â environment, use the the drop-down menu at the top left of the cloud shell pane to change it toÂ **_PowerShell_**.
    
3. Note that you can resize the cloud shell by dragging the separator bar at the top of the pane, or by using theÂ **â€”**,Â **â—»**, andÂ **X**Â icons at the top right of the pane to minimize, maximize, and close the pane. For more information about using the Azure Cloud Shell, see theÂ [Azure Cloud Shell documentation](https://docs.microsoft.com/azure/cloud-shell/overview).
    
4. In the PowerShell pane, enter the following commands to clone this repo:
    
    CodeCopy
    
    ```
     rm -r dp-203 -f
     git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```
    
5. After the repo has been cloned, enter the following commands to change to the folder for this exercise and run theÂ **setup.ps1**Â script it contains:
    
    CodeCopy
    
    ```
     cd dp-203/Allfiles/labs/14
     ./setup.ps1
    ```
    
6. If prompted, choose which subscription you want to use (this will only happen if you have access to multiple Azure subscriptions).
7. When prompted, enter a suitable password to be set for your Azure Synapse SQL pool.
    
    > **Note**: Be sure to remember this password!
    
8. Wait for the script to complete - this typically takes around 10 minutes, but in some cases may take longer. While you are waiting, review theÂ [What is Azure Synapse Link for Azure Cosmos DB?](https://docs.microsoft.com/azure/cosmos-db/synapse-link)Â article in the Azure Synapse Analytics documentation.

## Configure Synapse Link in Azure Cosmos DB

Before you can use Synapse Link for Azure Cosmos DB, you must enable it in your Azure Cosmos DB account and configure a container as an analytical store.

### Enable the Synapse Link feature in your Cosmos DB account

1. In theÂ [Azure portal](https://portal.azure.com/), browse to theÂ **dp203-_xxxxxxx_**Â resource group that was created by the setup script, and identify yourÂ **cosmos_xxxxxxxx_**Â Cosmos DB account.
    
    > **Note**: In some cases, the script may have tried to create Cosmos DB accounts in multiple regions, so there may be one or more accounts in aÂ _deleting_Â state. The active account should be the one with the largest number at the end of its name - for exampleÂ **cosmos_xxxxxxx_3**.
    
2. Open your Azure Cosmos DB account, and select theÂ **Data Explorer**Â page on the left side of its blade.
    
    _If aÂ **Welcome**Â dialog box is displayed, close it_
    
3. At the top of theÂ **Data Explorer**Â page, use theÂ **Enable Azure Synapse Link**Â button to enable Synapse Link.
    
    [![Cosmos DB Data Explorer with Enable Azure Synapse Link button highlighted](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-enable-synapse-link.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-enable-synapse-link.png)
    
4. On the left side of the page, in theÂ **Integrations**Â section, select theÂ **Azure Synapse Link**Â page and verify that the status of the account isÂ _Enabled_.
    

### Create an analytical store container

1. Return to theÂ **Data Explorer**Â page, and use theÂ **new Container**Â button (or tile) to create a new container with the following settings:
    
    - **Database id**:Â _(Create new)_Â AdventureWorks
    - **Share throughput across containers**:Â Unselected
    - **Container id**: Sales
    - **Partition key**: /customerid
    - **Container throughput (autoscale)**: Autoscale
    - **Container Max RU/s**: 4000
    - **Analytical store**: On
    
    > **Note**: In this scenario,Â **customerid**Â is used for partition key as itâ€™s likely to be used in many queries to retrieve customer and sales order information in a hypothetical application, it has relatively high cardinality (number of unique values), so it will allow the container to scale as the number of customers and sales orders grows. Using autoscale and setting the maximum value to 4000 RU/s is appropriate for a new application with initially low query volumes. A max value 4000 RU/s will enable the container to automatically scale between this value all the way down to 10% of this max value (400 RU/s) when not needed.
    
2. After the container has been created, in theÂ **Data Explorer**Â page, expand theÂ **AdventureWorks**Â database and itsÂ **Sales**Â folder; and then select theÂ **Items**Â folder.
    
    [![The Adventure Works, Sales, Items folder in Data Explorer](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-items-folder.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-items-folder.png)
    
3. Use theÂ **New Item**Â button to create a new customer item based on the following JSON. Then save the new item (some additional metadata fields will be added when you save the item).
    
    CodeCopy
    
    ```json
     {
         "id": "SO43701",
         "orderdate": "2019-07-01",
         "customerid": 123,
         "customerdetails": {
             "customername": "Christy Zhu",
             "customeremail": "christy12@adventure-works.com"
         },
         "product": "Mountain-100 Silver, 44",
         "quantity": 1,
         "price": 3399.99
     }
    ```
    
4. Add a second item with the following JSON:
    
    CodeCopy
    
    ```json
     {
         "id": "SO43704",
         "orderdate": "2019-07-01",
         "customerid": 124,
         "customerdetails": {
             "customername": "Julio Ruiz",
             "customeremail": "julio1@adventure-works.com"
         },
         "product": "Mountain-100 Black, 48",
         "quantity": 1,
         "price": 3374.99
     }
    ```
    
5. Add a third item with the following JSON:
    
    CodeCopy
    
    ```json
     {
         "id": "SO43707",
         "orderdate": "2019-07-02",
         "customerid": 125,
         "customerdetails": {
             "customername": "Emma Brown",
             "customeremail": "emma3@adventure-works.com"
         },
         "product": "Road-150 Red, 48",
         "quantity": 1,
         "price": 3578.27
     }
    ```
    

> **Note**: In reality, the analytical store would contain a much larger volume of data, written to the store by an application. These few items will be sufficient to demonstrate the principle in this exercise.

## Configure Synapse Link in Azure Synapse Analytics

Now that you have prepared your Azure Cosmos DB account, you can configure Azure Synapse link for Azure Cosmos DB in your Azure Synapse Analytics workspace.

1. In the Azure portal, close the blade for your Cosmos DB account if it is still open, and return to theÂ **dp203-_xxxxxxx_**Â resource group.
2. Open theÂ **synapse_xxxxxxx_**Â Synapse workspace, and on itsÂ **Overview**Â page, in theÂ **Open Synapse Studio**Â card, selectÂ **Open**Â to open Synapse Studio in a new browser tab; signing in if prompted.
3. On the left side of Synapse Studio, use theÂ **â€ºâ€º**Â icon to expand the menu - this reveals the different pages within Synapse Studio.
4. On theÂ **Data**Â page, view theÂ **Linked**Â tab. Your workspace should already include a link to your Azure Data Lake Storage Gen2 storage account, but no link to your Cosmos DB account.
5. In theÂ **+**Â menu, selectÂ **Connect to external data**, and then selectÂ **Azure Cosmos DB for NoSQL**.
    
    [![Adding an Azure Cosmos DB NoSQl API external data link](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/add-cosmos-db-link.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/add-cosmos-db-link.png)
    
6. Continue, and create a new Cosmos DB connection with the following settings:
    - **Name**: AdventureWorks
    - **Description**: AdventureWorks Cosmos DB database
    - **Connect via integration runtime**: AutoResolveIntegrationRuntime
    - **Authentication type**: Account key
    - **Connection string**:Â _selected_
    - **Account selection method**: From subscription
    - **Azure subscription**:Â _select your Azure subscription_
    - **Azure Cosmos DB account name**:Â _select yourÂ **cosmosxxxxxxx**Â account_
    - **Database name**: AdventureWorks
7. After creating the connection, use theÂ **â†»**Â button at the top right of theÂ **Data**Â page to refresh the view until anÂ **Azure Cosmos DB**Â category is listed in theÂ **Linked**Â pane.
8. Expand theÂ **Azure Cosmos DB**Â category to see theÂ **AdventureWorks**Â connection you created and theÂ **Sales**Â container it contains.
    
    [![Adding an Azure Cosmos DB SQl API external data link](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-linked-connection.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cosmos-linked-connection.png)
    

## Query Azure Cosmos DB from Azure Synapse Analytics

Now youâ€™re ready to query your Cosmos DB database from Azure Synapse Analytics.

### Query Azure Cosmos DB from a Spark pool

1. In theÂ **Data**Â pane, select theÂ **Sales**Â container, and in itsÂ **â€¦**Â menu, selectÂ **New Notebook**Â >Â **Load to DataFrame**.
2. In the newÂ **Notebook 1**Â tab that opens, in theÂ **Attach to**Â list, select your Spark pool (**spark_xxxxxxx_**). Then use theÂ **â–· Run all**Â button to run all of the cells in the notebook (thereâ€™s currently only one!).
    
    Since this is the first time youâ€™ve run any Spark code in this session, the Spark pool must be started. This means that the first run in the session can take a few minutes. Subsequent runs will be quicker.
    
3. While you are waiting for the Spark session to initialize, review the code that was generated (you can use theÂ **Properties**Â button, which looks similar toÂ **ğŸ—***, on the right end of the toolbar to close theÂ **Properties**Â pane so you can see the code more clearly). The code should look similar to this:
    
    CodeCopy
    
    ```python
     # Read from Cosmos DB analytical store into a Spark DataFrame and display 10 rows from the DataFrame
     # To select a preferred list of regions in a multi-region Cosmos DB account, add .option("spark.cosmos.preferredRegions", "<Region1>,<Region2>")
    
     df = spark.read\
         .format("cosmos.olap")\
         .option("spark.synapse.linkedService", "AdventureWorks")\
         .option("spark.cosmos.container", "Sales")\
         .load()
    
     display(df.limit(10))
    ```
    
4. When the code has finished running, and then review the output beneath the cell in the notebook. The results should include three records; one for each of the items you added to the Cosmos DB database. Each record includes the fields you entered when you created the items as well as some of the metadata fields that were automatically generated.
5. Under the results from the previous cell, use theÂ **+ Code**Â icon to add a new cell to the notebook, and then enter the following code in it:
    
    CodeCopy
    
    ```python
     customer_df = df.select("customerid", "customerdetails")
     display(customer_df)
    ```
    
6. Use theÂ **â–·**Â icon to the left of the cell to run it, and view the results; which should be similar to this:
    
    |customerid|customerdetails|
    |---|---|
    |124|â€{â€œcustomernameâ€: â€œJulio Ruizâ€,â€customeremailâ€: â€œjulio1@adventure-works.comâ€}â€|
    |125|â€{â€œcustomernameâ€: â€œEmma Brownâ€,â€customeremailâ€: â€œemma3@adventure-works.comâ€}â€|
    |123|â€{â€œcustomernameâ€: â€œChristy Zhuâ€,â€customeremailâ€: â€œchristy12@adventure-works.comâ€}â€|
    
    This query created a new dataframe containing only theÂ **customerid**Â andÂ **customerdetails**Â columns. Observe that theÂ **customerdetails**Â column contains the JSON structure for the nested data in the source item. In the table of results that is displayed, you can use theÂ **â–º**Â icon next to the JSON value to expand it and see the individual fields it contains.
    
7. Add another new code cell and enter the following code:
    
    CodeCopy
    
    ```python
     customerdetails_df = df.select("customerid", "customerdetails.*")
     display(customerdetails_df)
    ```
    
8. Run the cell and review the results, which should include theÂ **customername**Â andÂ **customeremail**Â from theÂ **customerdetails**Â value as columns:
    
    |customerid|customername|customeremail|
    |---|---|---|
    |124|Julio Ruiz|julio1@adventure-works.com|
    |125|Emma Brown|emma3@adventure-works.com|
    |123|Christy Zhu|christy12@adventure-works.com|
    
    Spark enables you to run complex data manipulation code to restructure and explore the data from Cosmos DB. In this case, the PySpark language enables you to navigate the JSON properties hierarchy to retrieve the child fields of theÂ **customerdetails**Â field.
    
9. Add another new code cell and enter the following code:
    
    SqlCopy
    
    ```sql
     %%sql
    
     -- Create a logical database in the Spark metastore
     CREATE DATABASE salesdb;
    
     USE salesdb;
    
     -- Create a table from the Cosmos DB container
     CREATE TABLE salesorders using cosmos.olap options (
         spark.synapse.linkedService 'AdventureWorks',
         spark.cosmos.container 'Sales'
     );
    
     -- Query the table
     SELECT *
     FROM salesorders;
    ```
    
10. Run the new cell to create a new database containing a table that includes data from the Cosmos DB analytical store.
11. Add another new code cell, and then enter and run the following code:
    
    SqlCopy
    
    ```sql
    %%sql
    
    SELECT id, orderdate, customerdetails.customername, product
    FROM salesorders
    ORDER BY id;
    ```
    
    The results from this query should resemble this:
    
    |id|orderdate|customername|product|
    |---|---|---|---|
    |SO43701|2019-07-01|Christy Zhu|Mountain-100 Silver, 44|
    |SO43704|2019-07-01|Julio Ruiz|Mountain-100 Black, 48|
    |SO43707|2019-07-02|Emma Brown|Road-150 Red, 48|
    
    Observe that when using Spark SQL, you can retrieve named properties of a JSON structure as columns.
    
12. Keep theÂ **Notebook 1**Â tab open - youâ€™ll return to it later.

### Query Azure Cosmos DB from a serverless SQL pool

1. In theÂ **Data**Â pane, select theÂ **Sales**Â container, and in itsÂ **â€¦**Â menu, selectÂ **New SQL script**Â >Â **Select TOP 100 rows**.
2. In theÂ **SQL script 1**Â tab that opens, hide theÂ **Properties**Â pane and view the code that has been generated, which should look similar to this:
    
    SqlCopy
    
    ```sql
     IF (NOT EXISTS(SELECT * FROM sys.credentials WHERE name = 'cosmosxxxxxxxx'))
     THROW 50000, 'As a prerequisite, create a credential with Azure Cosmos DB key in SECRET option:
     CREATE CREDENTIAL [cosmosxxxxxxxx]
     WITH IDENTITY = ''SHARED ACCESS SIGNATURE'', SECRET = ''<Enter your Azure Cosmos DB key here>''', 0
     GO
    
     SELECT TOP 100 *
     FROM OPENROWSET(â€‹PROVIDER = 'CosmosDB',
                     CONNECTION = 'Account=cosmosxxxxxxxx;Database=AdventureWorks',
                     OBJECT = 'Sales',
                     SERVER_CREDENTIAL = 'cosmosxxxxxxxx'
     ) AS [Sales]
    ```
    
    The SQL pool requires a credential to use when accessing Cosmos DB, which is based on an authorization key for your Cosmos DB account. The script includes an initialÂ `IF (NOT EXISTS(...`Â statement that checks for this credential, and throws an error if it does not exist.
    
3. Replace theÂ `IF (NOT EXISTS(...`Â statement in the script with the following code to create a credential, replacingÂ _cosmosxxxxxxxx_Â with the name of your Cosmos DB account:
    
    SqlCopy
    
    ```sql
     CREATE CREDENTIAL [cosmosxxxxxxxx]
     WITH IDENTITY = 'SHARED ACCESS SIGNATURE',
     SECRET = '<Enter your Azure Cosmos DB key here>'
     GO
    ```
    
    The whole script should now resemble the following:
    
    SqlCopy
    
    ```sql
     CREATE CREDENTIAL [cosmosxxxxxxxx]
     WITH IDENTITY = 'SHARED ACCESS SIGNATURE',
     SECRET = '<Enter your Azure Cosmos DB key here>'
     GO
    
     SELECT TOP 100 *
     FROM OPENROWSET(â€‹PROVIDER = 'CosmosDB',
                     CONNECTION = 'Account=cosmosxxxxxxxx;Database=AdventureWorks',
                     OBJECT = 'Sales',
                     SERVER_CREDENTIAL = 'cosmosxxxxxxxx'
     ) AS [Sales]
    ```
    
4. Switch to the browser tab containing the Azure portal (or open a new tab and sign into the Azure portal atÂ [https://portal.azure.com](https://portal.azure.com/)). Then in theÂ **dp203-_xxxxxxx_**Â resource group, open yourÂ **cosmos_xxxxxxxx_**Â Azure Cosmos DB account.
5. In the pane on the left, in theÂ **Settings**Â section, select theÂ **Keys**Â page. Then copy theÂ **Primary Key**Â value to the clipboard.
6. Switch back to the browser tab containing the SQL script in Azure Synapse Studio, and paste the key into the code replacing theÂ **_<Enter your Azure Cosmos DB key here>_**Â placeholder so that the script looks similar to this:
    
    SqlCopy
    
    ```sql
     CREATE CREDENTIAL [cosmosxxxxxxxx]
     WITH IDENTITY = 'SHARED ACCESS SIGNATURE',
     SECRET = '1a2b3c....................................=='
     GO
    
     SELECT TOP 100 *
     FROM OPENROWSET(â€‹PROVIDER = 'CosmosDB',
                     CONNECTION = 'Account=cosmosxxxxxxxx;Database=AdventureWorks',
                     OBJECT = 'Sales',
                     SERVER_CREDENTIAL = 'cosmosxxxxxxxx'
     ) AS [Sales]
    ```
    
7. Use theÂ **â–· Run**Â button to run the script, and review the results, which should include three records; one for each of the items you added to the Cosmos DB database.
    
    Now that you have created the credential, you can use it in any query against the Cosmos DB data source.
    
8. Replace all of the code in the script (both the CREATE CREDENTIAL and SELECT statements) with the following code (substitutingÂ _cosmosxxxxxxxx_Â with the name of your Azure Cosmos DB account):
    
    SqlCopy
    
    ```sql
     SELECT *
     FROM OPENROWSET(â€‹PROVIDER = 'CosmosDB',
                     CONNECTION = 'Account=cosmosxxxxxxxx;Database=AdventureWorks',
                     OBJECT = 'Sales',
                     SERVER_CREDENTIAL = 'cosmosxxxxxxxx'
     )
     WITH (
         OrderID VARCHAR(10) '$.id',
         OrderDate VARCHAR(10) '$.orderdate',
         CustomerID INTEGER '$.customerid',
         CustomerName VARCHAR(40) '$.customerdetails.customername',
         CustomerEmail VARCHAR(30) '$.customerdetails.customeremail',
         Product VARCHAR(30) '$.product',
         Quantity INTEGER '$.quantity',
         Price FLOAT '$.price'
     )
     AS sales
     ORDER BY OrderID;
    ```
    
9. Run the script and review the results, which should match the schema defined in theÂ `WITH`Â clause:
    
    |OrderID|OrderDate|CustomerID|CustomerName|CustomerEmail|Product|Quantity|Price|
    |---|---|---|---|---|---|---|---|
    |SO43701|2019-07-01|123|Christy Zhu|christy12@adventure-works.com|Mountain-100 Silver, 44|1|3399.99|
    |SO43704|2019-07-01|124|Julio Ruiz|julio1@adventure-works.com|Mountain-100 Black, 48|1|3374.99|
    |SO43707|2019-07-02|125|Emma Brown|emma3@adventure-works.com|Road-150 Red, 48|1|3578.27|
    
10. Keep theÂ **SQL script 1**Â tab open - youâ€™ll return to it later.

### Verify data modifications in Cosmos DB are reflected in Synapse

1. Leaving the browser tab containing Synapse Studio open, switch back to the tab containing the Azure portal, which should be open at theÂ **Keys**Â page for your Cosmos DB account.
2. On theÂ **Data Explorer**Â page, expand theÂ **AdventureWorks**Â database and itsÂ **Sales**Â folder; and then select theÂ **Items**Â folder.
3. Use theÂ **New Item**Â button to create a new customer item based on the following JSON. Then save the new item (some additional metadata fields will be added when you save the item).
    
    CodeCopy
    
    ```json
     {
         "id": "SO43708",
         "orderdate": "2019-07-02",
         "customerid": 126,
         "customerdetails": {
             "customername": "Samir Nadoy",
             "customeremail": "samir1@adventure-works.com"
         },
         "product": "Road-150 Black, 48",
         "quantity": 1,
         "price": 3578.27
     }
    ```
    
4. Return to the Synapse Studio tab and in theÂ **SQL Script 1**Â tab, re-run the query. Initially, it may show the same results as before, but wait a minute or so and then re-run the query again until the results include the sale to Samir Nadoy on 2019-07-02.
5. Switch back to theÂ **Notebook 1**Â tab and re-run the last cell in the Spark notebook to verify that the sale to Samir Nadoy is now included in the query results.

## Delete Azure resources

If youâ€™ve finished exploring Azure Synapse Analytics, you should delete the resources youâ€™ve created to avoid unnecessary Azure costs.

1. Close the Synapse Studio browser tab and return to the Azure portal.
2. On the Azure portal, on theÂ **Home**Â page, selectÂ **Resource groups**.
3. Select theÂ **dp203-_xxxxxxx_**Â resource group for your Synapse Analytics workspace (not the managed resource group), and verify that it contains the Synapse workspace, storage account, and Spark pool for your workspace as well as your Azure Cosmos DB account.
4. At the top of theÂ **Overview**Â page for your resource group, selectÂ **Delete resource group**.
5. Enter theÂ **dp203-_xxxxxxx_**Â resource group name to confirm you want to delete it, and selectÂ **Delete**.
    
    After a few minutes, your Azure Synapse workspace resource group and the managed workspace resource group associated with it will be deleted.