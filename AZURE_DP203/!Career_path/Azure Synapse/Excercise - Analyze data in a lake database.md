# Analyze data in a lake database

Azure Synapse Analytics enables you to combine the flexibility of file storage in a data lake with the structured schema and SQL querying capabilities of a relational database through the ability to create aÂ _lake database_. A lake database is a relational database schema defined on a data lake file store that enables data storage to be separated from the compute used to query it. Lake databases combine the benefits of a structured schema that includes support for data types, relationships, and other features typically only found in relational database systems, with the flexibility of storing data in files that can be used independently of a relational database store. Essentially, the lake database â€œoverlaysâ€ a relational schema onto files in folders in the data lake.

This exercise should take approximatelyÂ **45**Â minutes to complete.

## Before you start

Youâ€™ll need anÂ [Azure subscription](https://azure.microsoft.com/free)Â in which you have administrative-level access.

## Provision an Azure Synapse Analytics workspace

To support a lake database, you need an Azure Synapse Analytics workspace with access to data lake storage. There is no need for a dedicated SQL pool, since you can define the lake database using the built-in serverless SQL pool. Optionally, you can also use a Spark pool to work with data in the lake database.

In this exercise, youâ€™ll use a combination of a PowerShell script and an ARM template to provision an Azure Synapse Analytics workspace.

1. Sign into theÂ [Azure portal](https://portal.azure.com/)Â atÂ `https://portal.azure.com`.
2. Use theÂ **[>_]**Â button to the right of the search bar at the top of the page to create a new Cloud Shell in the Azure portal, selecting aÂ **_PowerShell_**Â environment and creating storage if prompted. The cloud shell provides a command line interface in a pane at the bottom of the Azure portal, as shown here:
    
    [![Azure portal with a cloud shell pane](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)
    
    > **Note**: If you have previously created a cloud shell that uses aÂ _Bash_Â environment, use the drop-down menu at the top left of the cloud shell pane to change it toÂ **_PowerShell_**.
    
3. Note that you can resize the cloud shell by dragging the separator bar at the top of the pane, or by using theÂ **â€”**,Â **â—»**, andÂ **X**Â icons at the top right of the pane to minimize, maximize, and close the pane. For more information about using the Azure Cloud Shell, see theÂ [Azure Cloud Shell documentation](https://docs.microsoft.com/azure/cloud-shell/overview).
    
4. In the PowerShell pane, enter the following commands to clone this repo:
    
    CodeCopy
    
    ```
     rm -r dp-203 -f
     git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```
    
5. After the repo has been cloned, enter the following commands to change to the folder for this exercise and run theÂ **setup.ps1**Â script it contains:
    
    CodeCopy
    
    ```
     cd dp-203/Allfiles/labs/04
     ./setup.ps1
    ```
    
6. If prompted, choose which subscription you want to use (this will only happen if you have access to multiple Azure subscriptions).
7. When prompted, enter a suitable password to be set for your Azure Synapse SQL pool.
    
    > **Note**: Be sure to remember this password!
    
8. Wait for the script to complete - this typically takes around 10 minutes, but in some cases may take longer. While you are waiting, review theÂ [Lake database](https://docs.microsoft.com/azure/synapse-analytics/database-designer/concepts-lake-database)Â andÂ [Lake database templates](https://docs.microsoft.com/azure/synapse-analytics/database-designer/concepts-database-templates)Â articles in the Azure Synapse Analytics documentation.

## Modify container permissions

1. After the deployment script has been completed, in the Azure portal, go to theÂ **dp203-_xxxxxxx_**Â resource group that it created, and notice that this resource group contains your Synapse workspace, a Storage account for your data lake, and an Apache Spark pool.
2. Select theÂ **Storage account**Â for your data lake namedÂ **datalakexxxxxxx**
    
    [![Data lake navigation to container](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/datalakexxxxxx-storage.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/datalakexxxxxx-storage.png)
    
3. Within theÂ **datalakexxxxxx**Â container, select theÂ **files folder**
    
    [![Select the files folder within the data lake container](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/dp203-Container.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/dp203-Container.png)
    
4. Within theÂ **files folder**Â youâ€™ll note theÂ **Authentication method:**Â is listed asÂ **_Access key (Switch to Entra User Account)_**Â click on this to change to Entra User Account.
    
    [![Change to Azure AD user account](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/dp203-switch-to-aad-user.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/dp203-switch-to-aad-user.png)
    
    ## Create a lake database
    

A lake database is a type of database that you can define in your workspace, and work with using the built-in serverless SQL pool.

1. Select your Synapse workspace, and in itsÂ **Overview**Â page, in theÂ **Open Synapse Studio**Â card, selectÂ **Open**Â to open Synapse Studio in a new browser tab; signing in if prompted.
2. On the left side of Synapse Studio, use theÂ **â€ºâ€º**Â icon to expand the menu - this reveals the different pages within Synapse Studio that youâ€™ll use to manage resources and perform data analytics tasks.
3. On theÂ **Data**Â page, view theÂ **Linked**Â tab and verify that your workspace includes a link to your Azure Data Lake Storage Gen2 storage account.
4. On theÂ **Data**Â page, switch back to theÂ **Workspace**Â tab and note that there are no databases in your workspace.
5. In theÂ **+**Â menu, selectÂ **Lake database**Â to open a new tab in which you can design your database schema (accepting the database templates terms of use if prompted).
6. In theÂ **Properties**Â pane for the new database, change theÂ **Name**Â toÂ **RetailDB**Â and verify that theÂ **Input folder**Â property is automatically updated toÂ **files/RetailDB**. Leave theÂ **Data format**Â asÂ **Delimited Text**Â (you could also useÂ _Parquet_Â format, and you can override the file format for individual tables - weâ€™ll use comma-delimited data in this exercise.)
7. At the top of theÂ **RetailDB**Â pane, selectÂ **Publish**Â to save the database so far.
8. In theÂ **Data**Â pane on the left, view theÂ **Linked**Â tab. Then expandÂ **Azure Data Lake Storage Gen2**Â and the primaryÂ **datalake_xxxxxxx_**Â store for yourÂ **synapse_xxxxxxx_**Â workspace, and select theÂ **files**Â file system; which currently contains a folder namedÂ **synapse**.
9. In theÂ **files**Â tab that has opened, use theÂ **+ New folder**Â button to create a new folder namedÂ **RetailDB**Â - this will be the input folder for the data files used by tables in your database.

## Create a table

Now that you have created a lake database, you can define its schema by creating tables.

### Define the table schema

1. Switch back to theÂ **RetailDB**Â tab for your database definition, and in theÂ **+ Table**Â list, selectÂ **Custom**, and note that a new table namedÂ **Table_1**Â is added to your database.
2. WithÂ **Table_1**Â selected, in theÂ **General**Â tab under the database design canvas, change theÂ **Name**Â property toÂ **Customer**.
3. Expand theÂ **Storage settings for table**Â section and note that the table will be stored as delimited text in theÂ **files/RetailDB/Customer**Â folder in the default data lake store for your Synapse workspace.
4. On theÂ **Columns**Â tab, note that by default, the table contains one column namedÂ **Column_1**. Edit the column definition to match the following properties:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |CustomerId|PK ðŸ—¹|Unique customer ID|ðŸ—†|long||
    
5. In theÂ **+ Column**Â list, selectÂ **New column**, and modify the new column definition to add aÂ **FirstName**Â column to the table as follows:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |CustomerId|PK ðŸ—¹|Unique customer ID|ðŸ—†|long||
    |**FirstName**|**PK ðŸ—†**|**Customer first name**|**ðŸ—†**|**string**|**256**|
    
6. Add more new columns until the table definition looks like this:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |CustomerId|PK ðŸ—¹|Unique customer ID|ðŸ—†|long||
    |FirstName|PK ðŸ—†|Customer first name|ðŸ—†|string|256|
    |LastName|PK ðŸ—†|Customer last name|ðŸ—¹|string|256|
    |EmailAddress|PK ðŸ—†|Customer email|ðŸ—†|string|256|
    |Phone|PK ðŸ—†|Customer phone|ðŸ—¹|string|256|
    
7. When youâ€™ve added all of the columns, publish the database again to save the changes.
8. In theÂ **Data**Â pane on the left, switch back to theÂ **Workspace**Â tab so you can see theÂ **RetailDB**Â lake database. Then expand it and refresh itsÂ **Tables**Â folder to see the newly createdÂ **Customer**Â table.

### Load data into the tableâ€™s storage path

1. In the main pane, switch back to theÂ **files**Â tab, which contains the file system with theÂ **RetailDB**Â folder. Then open theÂ **RetailDB**Â folder and create a new folder namedÂ **Customer**Â in it. This is where theÂ **Customer**Â table will get its data.
2. Open the newÂ **Customer**Â folder, which should be empty.
3. Download theÂ **customer.csv**Â data file fromÂ [https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/customer.csv](https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/customer.csv)Â and save it in a folder on your local computer (it doesnâ€™t matter where). Then in theÂ **Customer**Â folder in Synapse Explorer, use theÂ **â¤’ Upload**Â button to upload theÂ **customer.csv**Â file to theÂ **RetailDB/Customer**Â folder in your data lake.
    
    > **Note**: In a real production scenario, you would probably create a pipeline to ingest data into the folder for the table data. Weâ€™re uploading it directly in the Synapse Studio user interface in this exercise for expediency.
    
4. In theÂ **Data**Â pane on the left, on theÂ **Workspace**Â tab, in theÂ **â€¦**Â menu for theÂ **Customer**Â table, selectÂ **New SQL script**Â >Â **Select TOP 100 rows**. Then, in the newÂ **SQL script 1**Â pane that has opened, ensure that theÂ **Built-in**Â SQL pool is connected, and use theÂ **â–· Run**Â button to run the SQL code. The results should include first 100 rows from theÂ **Customer**Â table, based on the data stored in the underlying folder in the data lake.
5. Close theÂ **SQL script 1**Â tab, discarding your changes.

## Create a table from a database template

As youâ€™ve seen, you can create the tables you need in your lake database from scratch. However, Azure Synapse Analytics also provides numerous database templates based on common database workloads and entities that you can use as a starting point for your database schema.

### Define the table schema

1. In the main pane, switch back to theÂ **RetailDB**Â pane, which contains your database schema (currently containing only theÂ **Customer**Â table).
2. In theÂ **+ Table**Â menu, selectÂ **From template**. Then in theÂ **Add from template**Â page, selectÂ **Retail**Â and clickÂ **Continue**.
3. In theÂ **Add from template (Retail)**Â page, wait for the table list to populate, and then expandÂ **Product**Â and selectÂ **RetailProduct**. Then clickÂ **Add**. This adds a new table based on theÂ **RetailProduct**Â template to your database.
4. In theÂ **RetailDB**Â pane, select the newÂ **RetailProduct**Â table. Then, in the pane beneath the design canvas, on theÂ **General**Â tab, change the name toÂ **Product**Â and verify that the storage settings for the table specify the input folderÂ **files/RetailDB/Product**.
5. On theÂ **Columns**Â tab for theÂ **Product**Â table, note that the table already includes a large number of columns inherited from the template. There are more columns than required for this table, so youâ€™ll need to remove some.
6. Select the checkbox next toÂ **Name**Â to select all of the columns, and thenÂ unselect the following columns (which you need to retain):
    - ProductId
    - ProductName
    - IntroductionDate
    - ActualAbandonmentDate
    - ProductGrossWeight
    - ItemSku
7. On the toolbar in theÂ **Columns**Â pane, selectÂ **Delete**Â to remove the selected columns. This should leave you with the following columns:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |ProductId|PK ðŸ—¹|The unique identifier of a Product.|ðŸ—†|long||
    |ProductName|PK ðŸ—†|The name of the Productâ€¦|ðŸ—¹|string|128|
    |IntroductionDate|PK ðŸ—†|The date that the Product was introduced for sale.|ðŸ—¹|date|YYYY-MM-DD|
    |ActualAbandonmentDate|PK ðŸ—†|The actual date that the marketing of the product was discontinuedâ€¦|ðŸ—¹|date|YYY-MM-DD|
    |ProductGrossWeight|PK ðŸ—†|The gross product weight.|ðŸ—¹|decimal|18,8|
    |ItemSku|PK ðŸ—†|The Stock Keeping Unit identifierâ€¦|ðŸ—¹|string|20|
    
8. Add a new column namedÂ **ListPrice**Â to the table as shown here:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |ProductId|PK ðŸ—¹|The unique identifier of a Product.|ðŸ—†|long||
    |ProductName|PK ðŸ—†|The name of the Productâ€¦|ðŸ—¹|string|128|
    |IntroductionDate|PK ðŸ—†|The date that the Product was introduced for sale.|ðŸ—¹|date|YYYY-MM-DD|
    |ActualAbandonmentDate|PK ðŸ—†|The actual date that the marketing of the product was discontinuedâ€¦|ðŸ—¹|date|YYY-MM-DD|
    |ProductGrossWeight|PK ðŸ—†|The gross product weight.|ðŸ—¹|decimal|18,8|
    |ItemSku|PK ðŸ—†|The Stock Keeping Unit identifierâ€¦|ðŸ—¹|string|20|
    |**ListPrice**|**PK ðŸ—†**|**The product price.**|**ðŸ—†**|**decimal**|**18,2**|
    
9. When youâ€™ve modified the columns as shown above, publish the database again to save the changes.
10. In theÂ **Data**Â pane on the left, switch back to theÂ **Workspace**Â tab so you can see theÂ **RetailDB**Â lake database. Then use theÂ **â€¦**Â menu for itsÂ **Tables**Â folder to refresh the view and see the newly createdÂ **Product**Â table.

### Load data into the tableâ€™s storage path

1. In the main pane, switch back to theÂ **files**Â tab, which contains the file system, and navigate to theÂ **files/RetailDB**Â folder, which currently contains theÂ **Customer**Â folder for the table you created previously.
2. In theÂ **RetailDB**Â folder, create a new folder namedÂ **Product**. This is where theÂ **Product**Â table will get its data.
3. Open the newÂ **Product**Â folder, which should be empty.
4. Download theÂ **product.csv**Â data file fromÂ [https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/product.csv](https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/product.csv)Â and save it in a folder on your local computer (it doesnâ€™t matter where). Then in theÂ **Product**Â folder in Synapse Explorer, use theÂ **â¤’ Upload**Â button to upload theÂ **product.csv**Â file to theÂ **RetailDB/Product**Â folder in your data lake.
5. In theÂ **Data**Â pane on the left, on theÂ **Workspace**Â tab, in theÂ **â€¦**Â menu for theÂ **Product**Â table, selectÂ **New SQL script**Â >Â **Select TOP 100 rows**. Then, in the newÂ **SQL script 1**Â pane that has opened, ensure that theÂ **Built-in**Â SQL pool is connected, and use theÂ **â–· Run**Â button to run the SQL code. The results should include first 100 rows from theÂ **Product**Â table, based on the data stored in the underlying folder in the data lake.
6. Close theÂ **SQL script 1**Â tab, discarding your changes.

## Create a table from existing data

So far, youâ€™ve created tables and then populated them with data. In some cases, you may already have data in a data lake from which you want to derive a table.

### Upload data

1. In the main pane, switch back to theÂ **files**Â tab, which contains the file system, and navigate to theÂ **files/RetailDB**Â folder, which currently contains theÂ **Customer**Â andÂ **Product**Â folders for the tables you created previously.
2. In theÂ **RetailDB**Â folder, create a new folder namedÂ **SalesOrder**.
3. Open the newÂ **SalesOrder**Â folder, which should be empty.
4. Download theÂ **salesorder.csv**Â data file fromÂ [https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/salesorder.csv](https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/04/data/salesorder.csv)Â and save it in a folder on your local computer (it doesnâ€™t matter where). Then in theÂ **SalesOrder**Â folder in Synapse Explorer, use theÂ **â¤’ Upload**Â button to upload theÂ **salesorder.csv**Â file to theÂ **RetailDB/SalesOrder**Â folder in your data lake.

### Create a table

1. In the main pane, switch back to theÂ **RetailDB**Â pane, which contains your database schema (currently containing theÂ **Customer**Â andÂ **Product**Â tables).
2. In theÂ **+ Table**Â menu, selectÂ **From data lake**. Then in theÂ **Create external table from data lake**Â pane, specify the following options:
    - **External table name**: SalesOrder
    - **Linked service**: SelectÂ **synapse_xxxxxxx_-WorkspaceDefautStorage(datalake_xxxxxxx_)**
    - **Input file of folder**: files/RetailDB/SalesOrder
3. Continue to the next page and then create the table with the following options:
    - **File type**: CSV
    - **Field terminator**: Default (comma ,)
    - **First row**: LeaveÂ _infer column names_Â unselected.
    - **String delimiter**: Default (Empty string)
    - **Use default type**: Default type (true,false)
    - **Max string length**: 4000
4. When the table has been created, note that it includes columns namedÂ **C1**,Â **C2**, and so on and that the data types have been inferred from the data in the folder. Modify the column definitions as follows:
    
    |Name|Keys|Description|Nullability|Data type|Format / Length|
    |---|---|---|---|---|---|
    |SalesOrderId|PK ðŸ—¹|The unique identifier of an order.|ðŸ—†|long||
    |OrderDate|PK ðŸ—†|The date of the order.|ðŸ—†|timestamp|yyyy-MM-dd|
    |LineItemId|PK ðŸ—¹|The ID of an individual line item.|ðŸ—†|long||
    |CustomerId|PK ðŸ—†|The customer.|ðŸ—†|long||
    |ProductId|PK ðŸ—†|The product.|ðŸ—†|long||
    |Quantity|PK ðŸ—†|The order quantity.|ðŸ—†|long||
    
    > **Note**: The table contains a record for each individual item ordered, and includes a composite primary key comprised ofÂ **SalesOrderId**Â andÂ **LineItemId**.
    
5. On theÂ **Relationships**Â tab for theÂ **SalesOrder**Â table, in theÂ **+ Relationship**Â list, selectÂ **To table**, and then define the following relationship:
    
    |From table|From column|To table|To column|
    |---|---|---|---|
    |Customer|CustomerId|SalesOrder|CustomerId|
    
6. Add a secondÂ _To table_Â relationship with the following settings:
    
    |From table|From column|To table|To column|
    |---|---|---|---|
    |Product|ProductId|SalesOrder|ProductId|
    
    The ability to define relationships between tables helps enforce referential integrity between related data entities. This is a common feature of relational databases that would otherwise be difficult to apply to files in a data lake.
    
7. Publish the database again to save the changes.
8. In theÂ **Data**Â pane on the left, switch back to theÂ **Workspace**Â tab so you can see theÂ **RetailDB**Â lake database. Then use theÂ **â€¦**Â menu for itsÂ **Tables**Â folder to refresh the view and see the newly createdÂ **SalesOrder**Â table.

## Work with lake database tables

Now that you have some tables in your database, you can use them to work with the underlying data.

### Query tables using SQL

1. In Synapse Studio, select theÂ **Develop**Â page.
2. In theÂ **Develop**Â pane, in theÂ **+**Â menu, selectÂ **SQL script**.
3. In the newÂ **SQL script 1**Â pane, ensure the script is connected to theÂ **Built-in**Â SQL pool and in theÂ **User database**Â list, selectÂ **RetailDB**.
4. Enter the following SQL code:
    
    SqlCopy
    
    ```sql
     SELECT o.SalesOrderID, c.EmailAddress, p.ProductName, o.Quantity
     FROM SalesOrder AS o
     JOIN Customer AS c ON o.CustomerId = c.CustomerId
     JOIN Product AS p ON o.ProductId = p.ProductId
    ```
    
5. Use theÂ **â–· Run**Â button to run the SQL code.
    
    The results show order details with customer and product information.
    
6. Close theÂ **SQL script 1**Â pane, discarding your changes.

### Insert data using Spark

1. In theÂ **Develop**Â pane, in theÂ **+**Â menu, selectÂ **Notebook**.
2. In the newÂ **Notebook 1**Â pane, attach the notebook to theÂ **spark_xxxxxxx_*** Spark pool.
3. Enter the following code in the empty notebook cell:
    
    CodeCopy
    
    ```
     %%sql
     INSERT INTO `RetailDB`.`SalesOrder` VALUES (99999, CAST('2022-01-01' AS TimeStamp), 1, 6, 5, 1)
    ```
    
4. Use theÂ **â–·**Â button on the left of the cell to run it and wait for it to finish running. Note that it will take some time to start the Spark pool.
5. Use theÂ **+ Code**Â button to add a new cell to the notebook.
6. Enter the following code in the new cell:
    
    CodeCopy
    
    ```
     %%sql
     SELECT * FROM `RetailDB`.`SalesOrder` WHERE SalesOrderId = 99999
    ```
    
7. Use theÂ **â–·**Â button on the left of the cell to run it and verify that a row for sales order 99999 was inserted into theÂ **SalesOrder**Â table.
8. Close theÂ **Notebook 1**Â pane, stopping the Spark session and discarding your changes.

## Delete Azure resources

If youâ€™ve finished exploring Azure Synapse Analytics, you should delete the resources youâ€™ve created to avoid unnecessary Azure costs.

1. Close the Synapse Studio browser tab and return to the Azure portal.
2. On the Azure portal, on theÂ **Home**Â page, selectÂ **Resource groups**.
3. Select theÂ **dp203-_xxxxxxx_**Â resource group for your Synapse Analytics workspace (not the managed resource group), and verify that it contains the Synapse workspace, storage account, and Spark pool for your workspace.
4. At the top of theÂ **Overview**Â page for your resource group, selectÂ **Delete resource group**.
5. Enter theÂ **dp203-_xxxxxxx_**Â resource group name to confirm you want to delete it, and selectÂ **Delete**.
    
    After a few minutes, your Azure Synapse workspace resource group and the managed workspace resource group associated with it will be deleted.