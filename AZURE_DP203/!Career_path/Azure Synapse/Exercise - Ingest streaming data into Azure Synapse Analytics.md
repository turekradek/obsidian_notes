# Ingest realtime data with Azure Stream Analytics and Azure Synapse Analytics

Data analytics solutions often include a requirement to ingest and processÂ _streams_Â of data. Stream processing differs from batch processing in that streams are generallyÂ _boundless_Â - in other words they are continuous sources of data that must be processed perpetually rather than at fixed intervals.

Azure Stream Analytics provides a cloud service that you can use to define aÂ _query_Â that operates on a stream of data from a streaming source, such as Azure Event Hubs or an Azure IoT Hub. You can use an Azure Stream Analytics query to ingest the stream of data directly into a data store for further analysis, or to filter, aggregate, and summarize the data based on temporal windows.

In this exercise, youâ€™ll use Azure Stream Analytics to process a stream of sales order data, such as might be generated from an online retail application. The order data will be sent to Azure Event Hubs, from where your Azure Stream Analytics jobs will read the data and ingest it into Azure Synapse Analytics.

This exercise should take approximatelyÂ **45**Â minutes to complete.

## Before you start

Youâ€™ll need anÂ [Azure subscription](https://azure.microsoft.com/free)Â in which you have administrative-level access.

## Provision Azure resources

In this exercise, youâ€™ll need an Azure Synapse Analytics workspace with access to data lake storage and a dedicated SQL pool. Youâ€™ll also need an Azure Event Hubs namespace to which the streaming order data can be sent.

Youâ€™ll use a combination of a PowerShell script and an ARM template to provision these resources.

1. Sign into theÂ [Azure portal](https://portal.azure.com/)Â atÂ `https://portal.azure.com`.
2. Use theÂ **[>_]**Â button to the right of the search bar at the top of the page to create a new Cloud Shell in the Azure portal, selecting aÂ **_PowerShell_**Â environment and creating storage if prompted. The cloud shell provides a command line interface in a pane at the bottom of the Azure portal, as shown here:
    
    [![Azure portal with a cloud shell pane](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)](https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/images/cloud-shell.png)
    
    > **Note**: If you have previously created a cloud shell that uses aÂ _Bash_Â environment, use the the drop-down menu at the top left of the cloud shell pane to change it toÂ **_PowerShell_**.
    
3. Note that you can resize the cloud shell by dragging the separator bar at the top of the pane, or by using theÂ **â€”**,Â **â—»**, andÂ **X**Â icons at the top right of the pane to minimize, maximize, and close the pane. For more information about using the Azure Cloud Shell, see theÂ [Azure Cloud Shell documentation](https://docs.microsoft.com/azure/cloud-shell/overview).
    
4. In the PowerShell pane, enter the following commands to clone the repo containing this exercise:
    
    CodeCopy
    
    ```
     rm -r dp-203 -f
     git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```
    
5. After the repo has been cloned, enter the following commands to change to the folder for this exercise and run theÂ **setup.ps1**Â script it contains:
    
    CodeCopy
    
    ```
     cd dp-203/Allfiles/labs/18
     ./setup.ps1
    ```
    
6. If prompted, choose which subscription you want to use (this will only happen if you have access to multiple Azure subscriptions).
7. When prompted, enter a suitable password to be set for your Azure Synapse SQL pool.
    
    > **Note**: Be sure to remember this password!
    
8. Wait for the script to complete - this typically takes around 15 minutes, but in some cases may take longer. While you are waiting, review theÂ [Welcome to Azure Stream Analytics](https://learn.microsoft.com/azure/stream-analytics/stream-analytics-introduction)Â article in the Azure Stream Analytics documentation.

## Ingest streaming data into a dedicated SQL pool

Letâ€™s start by ingesting a stream of data directly into a table in an Azure Synapse Analytics dedicated SQL pool.

### View the streaming source and database table

1. When the setup script has finished running, minimize the cloud shell pane (youâ€™ll return to it later). Then in the Azure portal, go to theÂ **dp203-_xxxxxxx_**Â resource group that it created, and notice that this resource group contains an Azure Synapse workspace, a Storage account for your data lake, a Dedicated SQL pool, and an Event Hubs namespace.
2. Select your Synapse workspace, and in itsÂ **Overview**Â page, in theÂ **Open Synapse Studio**Â card, selectÂ **Open**Â to open Synapse Studio in a new browser tab. Synapse Studio is a web-based interface that you can use to work with your Synapse Analytics workspace.
3. On the left side of Synapse Studio, use theÂ **â€ºâ€º**Â icon to expand the menu - this reveals the different pages within Synapse Studio that youâ€™ll use to manage resources and perform data analytics tasks.
4. On theÂ **Manage**Â page, in theÂ **SQL pools**Â section, select theÂ **sql_xxxxxxx_**Â dedicated SQL pool row and then use itsÂ **â–·**Â icon to resume it.
5. While you wait for the SQL pool to start, switch back to the browser tab containing the Azure portal and re-open the cloud shell pane.
6. In the cloud shell pane, enter the following command to run a client app that sends 100 simulated orders to Azure Event Hubs:
    
    CodeCopy
    
    ```
     node ~/dp-203/Allfiles/labs/18/orderclient
    ```
    
7. Observe the order data as it is sent - each order consists of a product ID and a quantity.
8. After the order client app has finished, minimize the cloud shell pane and switch back to the Synapse Studio browser tab.
9. In Synapse Studio, on theÂ **Manage**Â page, ensure that your dedicated SQL pool has a status ofÂ **Online**, then switch to theÂ **data**Â page and in theÂ **Workspace**Â pane, expandÂ **SQL database**, yourÂ **sql_xxxxxxx_**Â SQL pool, andÂ **Tables**Â to see theÂ **dbo.FactOrder**Â table.
10. In theÂ **â€¦**Â menu for theÂ **dbo.FactOrder**Â table, selectÂ **New SQL script**Â >Â **Select TOP 100 rows**Â and review the results. Observe that the table includes columns forÂ **OrderDateTime**,Â **ProductID**, andÂ **Quantity**Â but there are currently no rows of data.

### Create an Azure Stream Analytics job to ingest order data

1. Switch back to the browser tab containing the Azure portal, and note the region where yourÂ **dp203-_xxxxxxx_**Â resource group was provisioned - you will create your Stream Analytics job in theÂ same region.
2. On theÂ **Home**Â page selectÂ **+ Create a resource**Â and search forÂ `Stream Analytics job`. Then create aÂ **Stream Analytics job**Â with the following properties:
    - **Basics**:
        - **Subscription**: Your Azure subscription
        - **Resource group**: Select the existingÂ **dp203-_xxxxxxx_**Â resource group.
        - **Name**:Â `ingest-orders`
        - **Region**: Select theÂ same regionÂ where your Synapse Analytics workspace is provisioned.
        - **Hosting environment**: Cloud
        - **Streaming units**: 1
    - **Storage**:
        - **Add storage account**: Selected
        - **Subscription**: Your Azure subscription
        - **Storage accounts**: Select theÂ **datalake_xxxxxxx_**Â storage account
        - **Authentication mode**: Connection string
        - **Secure private data in storage account**: Selected
    - **Tags**:
        - _None_
3. Wait for deployment to complete and then go to the deployed Stream Analytics job resource.

### Create an input for the event data stream

1. On theÂ **ingest-orders**Â overview page, select theÂ **Inputs**Â page. Use theÂ **Add input**Â menu to add anÂ **Event Hub**Â input with the following properties:
    - **Input alias**:Â `orders`
    - **Select Event Hub from your subscriptions**: Selected
    - **Subscription**: Your Azure subscription
    - **Event Hub namespace**: Select theÂ **events_xxxxxxx_**Â Event Hubs namespace
    - **Event Hub name**: Select the existingÂ **eventhub_xxxxxxx_**Â event hub.
    - **Event Hub consumer group**: Select theÂ **Use existing**Â then select theÂ **$Default**Â consumer group
    - **Authentication mode**: Create system assigned managed identity
    - **Partition key**:Â _Leave blank_
    - **Event serialization format**: JSON
    - **Encoding**: UTF-8
2. Save the input and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create an output for the SQL table

1. View theÂ **Outputs**Â page for theÂ **ingest-orders**Â Stream Analytics job. Then use theÂ **Add output**Â menu to add anÂ **Azure Synapse Analytics**Â output with the following properties:
    - **Output alias**:Â `FactOrder`
    - **Select Azure Synapse Analytics from your subscriptions**: Selected
    - **Subscription**: Your Azure subscription
    - **Database**: Select theÂ **sql_xxxxxxx_Â (synapse_xxxxxxx_)**Â database
    - **Authentication mode**: SQL server authentication
    - **Username**: SQLUser
    - **Password**:Â _The password you specified for your SQL Pool when running the setup script_
    - **Table**:Â `FactOrder`
2. Save the output and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create a query to ingest the event stream

1. View theÂ **Query**Â page for theÂ **ingest-orders**Â Stream Analytics job. Then wait a few moments until the input preview is displayed (based on the sales order events previously captured in the event hub).
2. Observe that the input data includes theÂ **ProductID**Â andÂ **Quantity**Â fields in the messages submitted by the client app, as well as additional Event Hubs fields - including theÂ **EventProcessedUtcTime**Â field that indicates when the event was added to the event hub.
3. Modify the default query as follows:
    
    CodeCopy
    
    ```
     SELECT
         EventProcessedUtcTime AS OrderDateTime,
         ProductID,
         Quantity
     INTO
         [FactOrder]
     FROM
         [orders]
    ```
    
    Observe that this query takes fields from the input (event hub) and writes them directly to the output (SQL table).
    
4. Save the query.

### Run the streaming job to ingest order data

1. View theÂ **Overview**Â page for theÂ **ingest-orders**Â Stream Analytics job, and on theÂ **Properties**Â tab review theÂ **Inputs**,Â **Query**,Â **Outputs**, andÂ **Functions**Â for the job. If the number ofÂ **Inputs**Â andÂ **Outputs**Â is 0, use theÂ **â†» Refresh**Â button on theÂ **Overview**Â page to display theÂ **orders**Â input andÂ **FactTable**Â output.
2. Select theÂ **â–· Start**Â button, and start the streaming job now. Wait until you are notified that the streaming job started successfully.
3. Re-open the cloud shell pane and re-run the following command to submit another 100 orders.
    
    CodeCopy
    
    ```
     node ~/dp-203/Allfiles/labs/18/orderclient
    ```
    
4. While the order client app is running, switch to the Synapse Studio browser tab and view the query you previously ran to select the TOP 100 rows from theÂ **dbo.FactOrder**Â table.
5. Use theÂ **â–· Run**Â button to re-run the query and verify that the table now contains order data from the event stream (if not, wait a minute and re-run the query again). The Stream Analytics job will push all new event data into the table as long as the job is running and order events are being sent to the event hub.
6. On theÂ **Manage**Â page, pause theÂ **sql_xxxxxxx_**Â dedicated SQL pool (to prevent unnecessary Azure charges).
7. Return to the browser tab containing the Azure Portal and minimize the cloud shell pane. Then use theÂ **ðŸ—† Stop**Â button to stop the Stream Analytics job and wait for the notification that the Stream Analytics job has stopped successfully.

## Summarize streaming data in a data lake

So far, youâ€™ve seen how to use a Stream Analytics job to ingest messages from a streaming source into a SQL table. Now letâ€™s explore how to use Azure Stream Analytics to aggregate data over temporal windows - in this case, to calculate the total quantity of each product sold every 5 seconds. Weâ€™ll also explore how to use a different kind of output for the job by writing the results in CSV format in a data lake blob store.

### Create an Azure Stream Analytics job to aggregate order data

1. In the Azure portal, on theÂ **Home**Â page selectÂ **+ Create a resource**Â and search forÂ `Stream Analytics job`. Then create aÂ **Stream Analytics job**Â with the following properties:
    - **Basics**:
        - **Subscription**: Your Azure subscription
        - **Resource group**: Select the existingÂ **dp203-_xxxxxxx_**Â resource group.
        - **Name**:Â `aggregate-orders`
        - **Region**: Select theÂ same regionÂ where your Synapse Analytics workspace is provisioned.
        - **Hosting environment**: Cloud
        - **Streaming units**: 1
    - **Storage**:
        - **Add storage account**: Selected
        - **Subscription**: Your Azure subscription
        - **Storage accounts**: Select theÂ **datalake_xxxxxxx_**Â storage account
        - **Authentication mode**: Connection string
        - **Secure private data in storage account**: Selected
    - **Tags**:
        - _None_
2. Wait for deployment to complete and then go to the deployed Stream Analytics job resource.

### Create an input for the raw order data

1. On theÂ **aggregate-orders**Â overview page, select theÂ **Inputs**Â page. Use theÂ **Add input**Â menu to add anÂ **Event Hub**Â input with the following properties:
    - **Input alias**:Â `orders`
    - **Select Event Hub from your subscriptions**: Selected
    - **Subscription**: Your Azure subscription
    - **Event Hub namespace**: Select theÂ **events_xxxxxxx_**Â Event Hubs namespace
    - **Event Hub name**: Select the existingÂ **eventhub_xxxxxxx_**Â event hub.
    - **Event Hub consumer group**: Select the existingÂ **$Default**Â consumer group
    - **Authentication mode**: Create system assigned managed identity
    - **Partition key**:Â _Leave blank_
    - **Event serialization format**: JSON
    - **Encoding**: UTF-8
2. Save the input and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create an output for the data lake store

1. View theÂ **Outputs**Â page for theÂ **aggregate-orders**Â Stream Analytics job. Then use theÂ **Add output**Â menu to add aÂ **Blob storage/ADLS Gen2**Â output with the following properties:
    - **Output alias**:Â `datalake`
    - **Select Select Blob storage/ADLS Gen2 from your subscriptions from your subscriptions**: Selected
    - **Subscription**: Your Azure subscription
    - **Storage account**: Select theÂ **datalake_xxxxxxx_**Â storage account
    - **Container**: SelectÂ **Use existing**, and from the list select theÂ **files**Â container
    - **Authentication mode**: Connection string
    - **Event serialization format**: CSV - Comma (,)
    - **Encoding**: UTF-8
    - **Write mode**: Append,as results arrive
    - **Path pattern**:Â `{date}`
    - **Date format**: YYYY/MM/DD
    - **Time format**:Â _Not applicable_
    - **Minimum rows**: 20
    - **Maximum time**: 0 Hours, 1 minutes, 0 seconds
2. Save the output and wait while it is created. You will see several notifications. Wait for aÂ **Successful connection test**Â notification.

### Create a query to aggregate the event data

1. View theÂ **Query**Â page for theÂ **aggregate-orders**Â Stream Analytics job.
2. Modify the default query as follows:
    
    CodeCopy
    
    ```
     SELECT
         DateAdd(second,-5,System.TimeStamp) AS StartTime,
         System.TimeStamp AS EndTime,
         ProductID,
         SUM(Quantity) AS Orders
     INTO
         [datalake]
     FROM
         [orders] TIMESTAMP BY EventProcessedUtcTime
     GROUP BY ProductID, TumblingWindow(second, 5)
     HAVING COUNT(*) > 1
    ```
    
    Observe that this query uses theÂ **System.Timestamp**Â (based on theÂ **EventProcessedUtcTime**Â field) to define the start and end of each 5 secondÂ _tumbling_Â (non-overlapping sequential) window in which the total quantity for each product ID is calculated.
    
3. Save the query.

### Run the streaming job to aggregate order data

1. View theÂ **Overview**Â page for theÂ **aggregate-orders**Â Stream Analytics job, and on theÂ **Properties**Â tab review theÂ **Inputs**,Â **Query**,Â **Outputs**, andÂ **Functions**Â for the job. If the number ofÂ **Inputs**Â andÂ **Outputs**Â is 0, use theÂ **â†» Refresh**Â button on theÂ **Overview**Â page to display theÂ **orders**Â input andÂ **datalake**Â output.
2. Select theÂ **â–· Start**Â button, and start the streaming job now. Wait until you are notified that the streaming job started successfully.
3. Re-open the cloud shell pane and re-run the following command to submit another 100 orders:
    
    CodeCopy
    
    ```
     node ~/dp-203/Allfiles/labs/18/orderclient
    ```
    
4. When the order app has finished, minimize the cloud shell pane. Then switch to the Synapse Studio browser tab and on theÂ **Data**Â page, on theÂ **Linked**Â tab, expandÂ **Azure Data Lake Storage Gen2**Â >Â **synapse_xxxxxxx_Â (primary - datalake_xxxxxxx_)**Â and select theÂ **files (Primary)**Â container.
5. If theÂ **files**Â container is empty, wait a minute or so and then use theÂ **â†» Refresh**Â to refresh the view. Eventually, a folder named for the current year should be displayed. This in turn contains folders for the month and day.
6. Select the folder for the year and on theÂ **New SQL script**Â menu, selectÂ **Select TOP 100 rows**. Then set theÂ **File type**Â toÂ **Text format**Â and apply the settings.
7. In the query pane that opens, modify the query to add aÂ `HEADER_ROW = TRUE`Â parameter as shown here:
    
    SqlCopy
    
    ```sql
     SELECT
         TOP 100 *
     FROM
         OPENROWSET(
             BULK 'https://datalakexxxxxxx.dfs.core.windows.net/files/2023/**',
             FORMAT = 'CSV',
             PARSER_VERSION = '2.0',
             HEADER_ROW = TRUE
         ) AS [result]
    ```
    
8. Use theÂ **â–· Run**Â button to run the SQL query and view the results, which show the quantity of each product ordered in five-second periods.
9. Return to the browser tab containing the Azure Portal and use theÂ **ðŸ—† Stop**Â button to stop the Stream Analytics job and wait for the notification that the Stream Analytics job has stopped successfully.

## Delete Azure resources

If youâ€™ve finished exploring Azure Stream Analytics, you should delete the resources youâ€™ve created to avoid unnecessary Azure costs.

1. Close the Azure Synapse Studio browser tab and return to the Azure portal.
2. On the Azure portal, on theÂ **Home**Â page, selectÂ **Resource groups**.
3. Select theÂ **dp203-_xxxxxxx_**Â resource group containing your Azure Synapse, Event Hubs, and Stream Analytics resources (not the managed resource group).
4. At the top of theÂ **Overview**Â page for your resource group, selectÂ **Delete resource group**.
5. Enter theÂ **dp203-_xxxxxxx_**Â resource group name to confirm you want to delete it, and selectÂ **Delete**.
    
    After a few minutes, the resources created in this exercise will be deleted.